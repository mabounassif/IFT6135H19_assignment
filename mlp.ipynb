{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1920,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self, size_layers=[784, 16, 16, 10], datapath=None, modelpath=None):\n",
    "        self.data = np.load(datapath)\n",
    "        self.size_layers = size_layers\n",
    "        \n",
    "        # The cache would be used to store forward feed activations and preactivations values \n",
    "        self.cache = {\n",
    "            'activations': [],\n",
    "            'preactivations': []\n",
    "        }\n",
    "        \n",
    "        # Data structure to hold the params of the model\n",
    "        self.theta = {\n",
    "            'b': [],\n",
    "            'w': []\n",
    "        }\n",
    "        \n",
    "        # Data structure to hold the gradients calculated during backprop\n",
    "        self.grads = {\n",
    "            'b': [],\n",
    "            'w': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1921,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_zero_weights(self):\n",
    "    size_next_layers = self.size_layers.copy()\n",
    "    size_next_layers.pop(0)\n",
    "    \n",
    "    for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "        self.theta['w'].append(np.zeros((size_next_layer, size_layer)))\n",
    "        self.theta['b'].append(np.zeros((1, size_next_layer)))\n",
    "        \n",
    "        self.grads['w'].append(np.zeros((size_next_layer, size_layer)))\n",
    "        self.grads['b'].append(np.zeros((1, size_next_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1922,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_normal_weights(self):\n",
    "    size_next_layers = self.size_layers.copy()\n",
    "    size_next_layers.pop(0)\n",
    "\n",
    "    for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "        self.theta['w'].append(np.random.normal(size=(size_next_layer, size_layer)))\n",
    "        self.theta['b'].append(np.zeros((1, size_next_layer)))\n",
    "\n",
    "        self.grads['w'].append(np.random.normal(size=(size_next_layer, size_layer)))\n",
    "        self.grads['b'].append(np.zeros((1, size_next_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1923,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_glorot_weights(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1924,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X):\n",
    "    '''\n",
    "    Forward propagation\n",
    "    params:\n",
    "        X: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of sample\n",
    "    returns: output is a N by o matrix where o is the output dimension specified\n",
    "    in mlp initialization\n",
    "    '''\n",
    "    n_layers = len(self.size_layers)\n",
    "    input_layer = X\n",
    "    \n",
    "    # Adding input in activations needed for gradient descent\n",
    "    self.cache['activations'].append(X)\n",
    "    \n",
    "    # Index stops before the last layer as we'd need to treat it differently\n",
    "    for layer_idx in range(n_layers - 2):\n",
    "        \n",
    "        # Multiply the input by the weights\n",
    "        pre_act_layer = np.matmul(input_layer,  self.theta['w'][layer_idx].transpose()) + self.theta['b'][layer_idx]\n",
    "        # Apply activation function\n",
    "        output_layer = activation(self, pre_act_layer)\n",
    "        \n",
    "        self.cache['preactivations'].append(pre_act_layer)\n",
    "        self.cache['activations'].append(output_layer)\n",
    "        \n",
    "        input_layer = output_layer\n",
    "    \n",
    "    # Final layer with softmax activation\n",
    "    # Multiply the input by the weights\n",
    "    pre_act_layer = np.matmul(input_layer, self.theta['w'][-1].transpose()) + self.theta['b'][-1]\n",
    "    # Apply activation function\n",
    "    output_layer = softmax(self, pre_act_layer)\n",
    "\n",
    "    self.cache['preactivations'].append(pre_act_layer)\n",
    "    self.cache['activations'].append(output_layer)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1925,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(self, input):\n",
    "    zero_aux = np.zeros(input.shape)\n",
    "    meta_z = np.stack((input , zero_aux), axis = -1)\n",
    "    \n",
    "    return np.max(meta_z, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1926,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_prime(self, input):\n",
    "    result = input.copy()\n",
    "    result[result<=0] = 0\n",
    "    result[result>0] = 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1927,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self, input):\n",
    "    '''\n",
    "    params:\n",
    "        input: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of each sample\n",
    "    returns: a N by k matrix representing the probability distribution \n",
    "        of each sample\n",
    "    '''\n",
    "    e_x = np.exp(input - np.max(input, axis=1, keepdims=1))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1928,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, predictions, targets):\n",
    "    '''\n",
    "    Takes a batch of predictions and gets the cross entropy\n",
    "    params:\n",
    "        predictions: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of each sample\n",
    "        targets: N by 1 matrix containing label of each one\n",
    "        where \n",
    "            N: number of samples\n",
    "    returns: a scalar which is the loss value            \n",
    "    '''\n",
    "    N = predictions.shape[0] # batch size\n",
    "    k = predictions.shape[1] # numb of classes\n",
    "    \n",
    "    cross_entropy_softmax_prime = np.multiply(np.eye(k)[targets], np.log(predictions))\n",
    "    return np.sum(-1*cross_entropy_softmax_prime)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1929,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_prime(self, predictions, targets):\n",
    "    N = predictions.shape[0] # batch size\n",
    "    k = predictions.shape[1] # numb of classes\n",
    "    \n",
    "    return np.sum(predictions - np.eye(k)[targets], axis=0, keepdims=1)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1930,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, Y):\n",
    "    '''\n",
    "    Backward propagation\n",
    "    params:\n",
    "        Y: N by 1 matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            column: is the label of the samples\n",
    "    '''\n",
    "    N = Y.shape[0]\n",
    "    n_layers = len(self.size_layers)\n",
    "    \n",
    "    delta = loss_prime(self, self.cache['activations'][-1], Y)\n",
    "    \n",
    "    self.grads['b'][-1] = delta\n",
    "    self.grads['w'][-1] = np.dot(delta.transpose(), np.sum(self.cache['activations'][-2], axis=0, keepdims=1)/N)\n",
    "    \n",
    "    for l in range(2, n_layers):\n",
    "        z = self.cache['preactivations'][-l]\n",
    "        o = activation_prime(self, z)\n",
    "        \n",
    "        delta = np.multiply(np.dot(delta, self.theta['w'][-l+1]), np.sum(o, axis=0, keepdims=1)/N)\n",
    "        \n",
    "        self.grads['b'][-l] = delta\n",
    "        self.grads['w'][-l] = np.dot(delta.transpose(), np.sum(self.cache['activations'][-l-1], axis=0, keepdims=1)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1931,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta, lmbda, batch_size):\n",
    "    n_layers = len(self.size_layers)\n",
    "    \n",
    "    for layer_idx in range(n_layers - 1):\n",
    "        grad_w = self.grads['w'][layer_idx]\n",
    "        grad_b = self.grads['b'][layer_idx]\n",
    "        w = self.theta['w'][layer_idx]\n",
    "        b = self.theta['b'][layer_idx]\n",
    "        \n",
    "        self.theta['w'][layer_idx] = w - (eta/batch_size)*grad_w - lmbda*w\n",
    "        self.theta['b'][layer_idx] = b - (eta/batch_size)*grad_b - lmbda*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1932,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, eta, epoch=20, batch_size=1000, lmbda=0.5):\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        train_set, valid_set, test_set = self.data\n",
    "        \n",
    "        X, Y = train_set\n",
    "        \n",
    "        mini_batches = [\n",
    "            (X[k:k+batch_size], Y[k:k+batch_size]) for k in range(0, len(X), batch_size)\n",
    "        ]\n",
    "        \n",
    "        for X, Y in mini_batches:\n",
    "            forward(mlp, X)\n",
    "            backward(mlp, Y)\n",
    "            update(mlp, eta, lmbda, batch_size=batch_size)\n",
    "        \n",
    "        Y_h = forward(mlp, X)\n",
    "        losses.append(loss(mlp, Y_h, Y))\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1933,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question1\n",
    "1.\n",
    "\n",
    "Initializing weights to zero, with 0.5 as learning rate and no minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1934,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bfa2208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datapath = 'data/mnist.pkl.npy'\n",
    "modelpath = 'model/mlp.pkl.npy'\n",
    "\n",
    "eta = 0.5\n",
    "batch_size = 1000\n",
    "epoch = 20\n",
    "lmbda = 0.005\n",
    "\n",
    "mlp = NN(datapath=datapath, modelpath=modelpath, size_layers=[784, 16, 16, 10])\n",
    "\n",
    "initialize_zero_weights(mlp)\n",
    "losses = train(mlp, eta, epoch=epoch, batch_size=batch_size, lmbda=lmbda)\n",
    "\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.xlabel('epochs') \n",
    "plt.title('Loss function')\n",
    "plt.axhline(0, color='white')\n",
    "plt.axvline(0, color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing normally distributed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1935,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bfa2dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datapath = 'data/mnist.pkl.npy'\n",
    "modelpath = 'model/mlp.pkl.npy'\n",
    "\n",
    "eta = 0.5\n",
    "batch_size = 1000\n",
    "epoch = 20\n",
    "lmbda = 0.005\n",
    "\n",
    "mlp = NN(datapath=datapath, modelpath=modelpath, size_layers=[784, 16, 16, 10])\n",
    "\n",
    "initialize_normal_weights(mlp)\n",
    "losses = train(mlp, eta, epoch=epoch, batch_size=batch_size, lmbda=lmbda)\n",
    "\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.xlabel('epochs') \n",
    "plt.title('Loss function')\n",
    "plt.axhline(0, color='white')\n",
    "plt.axvline(0, color='white')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
