{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3127,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self, size_layers=[784, 16, 16, 10], datapath=None, modelpath=None):\n",
    "        self.data = np.load(datapath)\n",
    "        self.size_layers = size_layers\n",
    "        \n",
    "        # The cache would be used to store forward feed activations and preactivations values \n",
    "        self.cache = {\n",
    "            'activations': [],\n",
    "            'preactivations': []\n",
    "        }\n",
    "        \n",
    "        # Data structure to hold the params of the model\n",
    "        self.theta = {\n",
    "            'b': [],\n",
    "            'w': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_zero_weights(self):\n",
    "    size_next_layers = self.size_layers.copy()\n",
    "    size_next_layers.pop(0)\n",
    "    \n",
    "    for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "        self.theta['w'].append(np.zeros((size_next_layer, size_layer)))\n",
    "        self.theta['b'].append(np.zeros((1, size_next_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_normal_weights(self):\n",
    "    size_next_layers = self.size_layers.copy()\n",
    "    size_next_layers.pop(0)\n",
    "\n",
    "    for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "        self.theta['w'].append(np.random.normal(size=(size_next_layer, size_layer)))\n",
    "        self.theta['b'].append(np.zeros((1, size_next_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_glorot_weights(self):\n",
    "    size_next_layers = self.size_layers.copy()\n",
    "    size_next_layers.pop(0)\n",
    "\n",
    "    for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "        d_l = np.sqrt(6/(size_layer+size_next_layer))\n",
    "        m_d_l = -1*d_l\n",
    "        \n",
    "        self.theta['w'].append(np.random.uniform(m_d_l, d_l, size=(size_next_layer, size_layer)))\n",
    "        self.theta['b'].append(np.zeros((1, size_next_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    '''\n",
    "    Forward propagation\n",
    "    params:\n",
    "        x: sample of k dimension\n",
    "    returns: output of the network of dimention specified\n",
    "    in mlp initialization\n",
    "    '''\n",
    "    n_layers = len(self.size_layers)\n",
    "    input_layer = x\n",
    "    \n",
    "    # Adding input in activations needed for gradient descent\n",
    "    self.cache['activations'] = [x]\n",
    "    self.cache['preactivations'] = [None]\n",
    "    \n",
    "    # Index stops before the last layer as we'd need to treat it differently\n",
    "    for layer_idx in range(n_layers - 2):\n",
    "        \n",
    "        # Multiply the input by the weights\n",
    "        pre_act_layer = np.matmul(input_layer,  self.theta['w'][layer_idx].transpose()) + self.theta['b'][layer_idx]\n",
    "        # Apply activation function\n",
    "        output_layer = activation(self, pre_act_layer)\n",
    "        \n",
    "        self.cache['preactivations'].append(pre_act_layer)\n",
    "        self.cache['activations'].append(output_layer)\n",
    "        \n",
    "        input_layer = output_layer\n",
    "    \n",
    "    # Final layer with softmax activation\n",
    "    # Multiply the input by the weights\n",
    "    pre_act_layer = np.matmul(input_layer, self.theta['w'][-1].transpose()) + self.theta['b'][-1]\n",
    "    # Apply activation function\n",
    "    output_layer = softmax(self, pre_act_layer)\n",
    "\n",
    "    self.cache['preactivations'].append(pre_act_layer)\n",
    "    self.cache['activations'].append(output_layer)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(self, x):\n",
    "    zero_aux = np.zeros(x.shape)\n",
    "    meta_z = np.stack((x , zero_aux), axis = -1)\n",
    "    \n",
    "    return np.max(meta_z, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_prime(self, x):\n",
    "    result = x.copy()\n",
    "    result[result<=0] = 0\n",
    "    result[result>0] = 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self, x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=1))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, predictions, targets):\n",
    "    '''\n",
    "    Takes a batch of predictions and gets the cross entropy\n",
    "    params:\n",
    "        predictions: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of each sample\n",
    "        targets: N by 1 matrix containing label of each one\n",
    "        where \n",
    "            N: number of samples\n",
    "    returns: a scalar which is the loss value            \n",
    "    '''\n",
    "    N = predictions.shape[0] # batch size\n",
    "    k = predictions.shape[1] # numb of classes\n",
    "    \n",
    "    return -1*np.sum(np.multiply(np.eye(k)[targets][0], np.log10(predictions)))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_prime(self, prediction, target):\n",
    "    k = prediction.shape[1] # numb of classes\n",
    "\n",
    "    return np.multiply(np.eye(k)[target], np.eye(k)[target] - prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, y):\n",
    "    '''\n",
    "    Backward propagation\n",
    "    params:\n",
    "        y: current label\n",
    "    '''\n",
    "    n_layers = len(self.size_layers)\n",
    "    \n",
    "    grads = {\n",
    "        'b': [None] * (n_layers-1),\n",
    "        'w': [None] * (n_layers-1)\n",
    "    }\n",
    "    \n",
    "    delta = loss_prime(self, self.cache['activations'][-1], y)\n",
    "    \n",
    "    grads['b'][-1] = delta\n",
    "    grads['w'][-1] = np.dot(delta.transpose(), self.cache['activations'][-2])\n",
    "    \n",
    "    for l in range(2, n_layers):\n",
    "        z = self.cache['preactivations'][-l]\n",
    "        o = activation_prime(self, z)\n",
    "        \n",
    "        delta = np.multiply(np.dot(delta, self.theta['w'][-l+1]), o)\n",
    "        \n",
    "        grads['b'][-l] = delta\n",
    "        grads['w'][-l] = np.dot(delta.transpose(), self.cache['activations'][-l-1])\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta, lmbda, grads):\n",
    "    n_layers = len(self.size_layers)\n",
    "    \n",
    "    for layer_idx in range(n_layers - 1):\n",
    "        grad_w = grads['w'][layer_idx]\n",
    "        grad_b = grads['b'][layer_idx]\n",
    "        w = self.theta['w'][layer_idx]\n",
    "        b = self.theta['b'][layer_idx]\n",
    "        \n",
    "        self.theta['w'][layer_idx] = w - eta*grad_w - lmbda*w\n",
    "        self.theta['b'][layer_idx] = b - eta*grad_b - lmbda*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(self, dataset, batch_size):\n",
    "        X, Y = dataset\n",
    "        \n",
    "        mini_batches = [\n",
    "            (X[k:k+batch_size], Y[k:k+batch_size]) for k in range(0, len(X), batch_size)\n",
    "        ]\n",
    "        \n",
    "        return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(self, y, y_h):\n",
    "    return 100*np.sum(np.argmax(Y_h, axis=1) == Y)/ Y_h.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3141,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_batch = []\n",
    "def train(self, eta, epoch=20, batch_size=1000, lmbda=0.5):\n",
    "    losses = {\n",
    "        'train': [],\n",
    "        'validation': []\n",
    "    }\n",
    "    \n",
    "    accuracies = {\n",
    "        'train': [],\n",
    "        'validation': []\n",
    "    }\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        train_set, valid_set, test_set = self.data\n",
    "        \n",
    "        train_mini_batches = create_mini_batches(self, train_set, batch_size=batch_size)\n",
    "        validate_mini_batches = create_mini_batches(self, valid_set, batch_size=batch_size)\n",
    "        \n",
    "        tmp_losses = {\n",
    "            'train': [],\n",
    "            'validation': []\n",
    "        }\n",
    "        \n",
    "        tmp_accuracies = {\n",
    "            'train': [],\n",
    "            'validation': []\n",
    "        }\n",
    "        \n",
    "        for X, Y in train_mini_batches:\n",
    "            tmp_grads = {\n",
    "                'w': [],\n",
    "                'b': []\n",
    "            }\n",
    "                \n",
    "            for x, y in zip(X, Y):\n",
    "                Y_h = forward(self, np.array([x]))\n",
    "                grads = backward(self, np.array([y]))\n",
    "                \n",
    "                tmp_grads['w'].append(grads['w'])\n",
    "                tmp_grads['b'].append(grads['b'])\n",
    "            \n",
    "#             w_grad = tmp_grads['w'][0]\n",
    "#             b_grad = tmp_grads['b'][0]\n",
    "            \n",
    "#             for x, y in zip(tmp_grads['w'][1:], tmp_grads['w'][2:]):\n",
    "#                 w_grad += x + y\n",
    "                \n",
    "#             for x, y in zip(tmp_grads['b'][1:], tmp_grads['b'][2:]):\n",
    "#                 b_grad += x + y\n",
    "                \n",
    "#             update(self, eta, lmbda, {\n",
    "#                 'w': [_w_grad / len(tmp_grads['w']) for _w_grad in w_grad] ,\n",
    "#                 'b': [_b_grad / len(tmp_grads['b']) for _b_grad in b_grad]\n",
    "#             })\n",
    "            \n",
    "            los = loss(self, Y_h, Y)\n",
    "            acc = calculate_accuracy(self, Y_h, Y)\n",
    "            \n",
    "            tmp_losses['train'].append(los)\n",
    "            tmp_accuracies['train'].append(acc)\n",
    "            \n",
    "        for X, Y in validate_mini_batches:\n",
    "            Y_h = forward(self, X)\n",
    "            \n",
    "            los = loss(self, Y_h, Y)\n",
    "            acc = calculate_accuracy(self, Y_h, Y)\n",
    "            \n",
    "            last_batch = [(X, Y, Y_h)]\n",
    "            \n",
    "            tmp_losses['validation'].append(los)\n",
    "            tmp_accuracies['validation'].append(acc)\n",
    "            \n",
    "        losses['validation'].append(np.mean(tmp_losses['validation']))\n",
    "        losses['train'].append(np.mean(tmp_losses['train']))\n",
    "        \n",
    "        accuracies['validation'].append(np.mean(tmp_accuracies['validation']))\n",
    "        accuracies['train'].append(np.mean(tmp_accuracies['train']))\n",
    "        \n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    train_set, valid_set, test_set = self.data\n",
    "\n",
    "    X, Y = test_set\n",
    "    Y_h = forward(self, X)\n",
    "    \n",
    "    return np.sum(np.argmax(Y_h, axis=1) != Y)/ X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plots(self, epoch, accuracies, losses):\n",
    "    x = range(epoch)\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(x, accuracies['validation'], 'g-')\n",
    "    ax2.plot(x, accuracies['train'], 'b-')\n",
    "\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('validation accuracies', color='g')\n",
    "    ax2.set_ylabel('train accuracies', color='b')\n",
    "    plt.title('Accuracies')\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(x, losses['validation'], 'g-')\n",
    "    ax2.plot(x, losses['train'], 'b-')\n",
    "\n",
    "    ax1.set_xlabel('epochs')\n",
    "    ax1.set_ylabel('validation loss', color='g')\n",
    "    ax2.set_ylabel('train loss', color='b')\n",
    "    plt.title('Loss functions')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Initializing weights to zero, with 0.5 as learning rate and no minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAAEZCAYAAAAaKBUaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu8XfOd//HXW1zCkETULRfEiCnaumRcxlQdZdpICZ0poh1U6bQMUp0q7W+mO7voD/NTVZ0yrSApRYoqnZRozVFtiVsoDSUqOHEJIm6lRD6/P9Z3JzvHOTnffZJ99t7nvJ+Px3qctb/7u9b+7B3WZ33X+q7vVxGBmZmZrdwajQ7AzMysFThhmpmZZXDCNDMzy+CEaWZmlsEJ08zMLIMTppmZWQYnTLN+RtJekh5pdBxm/Y38HKbZyklqBz4EbBYRbzc4HDNrELcwzVZC0lbAXsBSYGIffu6affVZZpbHCdNs5Y4E7gCmAUdVCiWNlnSdpIWSXpR0QdV7n5c0V9Krkv4gaadUvlTS1lX1LpN0elpvk9Qh6auSngWmShom6efpMxZJulHSyKrth0u6VNKC9P5Pq/b1dFW9EZKuTfv5k6QTq97bTdI9kl6R9Jykc+vxI5r1B06YZit3JHA5cAXwcUkbSxoE/Bx4AtgSGAlcBSDpEKAEHBERQyhapYu62XekpWJTYENgC+ALFP9/Tk2vtwDeBL5XVf9HwGBge2AT4NudP0DSGsCNwBxgBLAv8CVJH0tVzgfOi4ihwNbAjJwfxWwg8mUfs25I+jBFopoREYskPQ58BpgNbA6cEhFLU/Xfpr/HAmdHxL0AEfF4Tx9Ttb4UKEXEO8A7wFvAT6vi+RZwa1rfHBgPDI+IV1KV27vY/67A+yLijPT6CUkXA5OAWcDbwFhJ74uIF9N3M7MuuIVp1r2jgFkRUWkhXpnKRgFPViXLaqOAnpJkd16o7lQkaT1J/y1pvqRXgNuAoZIEjAYWVSXL7mwJjJD0cmUBvkbRIgU4BtgWeFjSXZI+0cvYzfo9tzDNuiBpXeBQYI10TxFgHWAo8DywhaRBEfFup02fBrbpZrd/Btarer15ql/Rucv6v1Eks90iYmG6F3ofRav0aWC4pKE9JM2ngSciYtuu3oyIecCnAST9E3CNpOER8eZK9mk2ILmFada1g4ElwHbAjmnZDvgN8EngWeCs1AocLGnPtN3FwFck7aLCNpK2SO/dD3xG0iBJ44GP9BDD+hT3LV+RNJzi3igAEfEs8Avg+6lz0FqSutrfXcCrqTPRuumzPyDpbwEk/bOkjVPdVyiSdueTADPDCdOsO0cCl0RER0QsTMvzFJ1uDgMOoGhJPkXRijsUICKuAc4Efgy8ClxH0ZEHYDJwIPAyRavup6yocwvzO8C6wIvA7ygSZHWdIyjudT5C0eo9qfO+Ugv4QGAn4E/AC8APgCGp3seBhyS9BpwHTPKzpmZdq/vABalH4T1AR0QcKGlf4ByKZP068NmIeFzSOsB0YBfgJeCwiHgy7eNrwOcoznxPiohZqXw8xUFlEHBxRJydysdQ9FrckOIS1hGpI4WZmdVJfz/e90ULczIwl+VnxhcCn46InSnOwv89lR8DvBQRYynOdCs/xvYUZ/TbU/QK/H661DWI4mx/fHrvcEnbpX2dDZyb7tu8nPZtZmb11a+P93VNmJJGARMo7utUus8vpeg4ATAMWJDWJ1I8HA5wLcXzYgAHAVdGxDsRMR+YB+wO7AbMi4j56WziKuCg1INwH+CatP00ivtRZmZWJwPheF/vXrLnAaew/H4JwOeBmZLepLjHs3sqH0nqMRgRS9LIIxtRPGx9Z9X2HakurNjDsCPtaziwuKrL/4Kq+mZmVh/9/nhftxampAOAhRExhxUfzj4Z2D8iRgOXUvzIdKpTEaux3MzM6mCgHO/r2cLcE5goaQLF8F1DJP0ceH9E3J3qzKDo+QfFGcMWwDMqBp4emkZX6aB4SLtiFMVZhDqVjwYWRMSLqZv9GumsYxTwTFcBSnIiNTPrhYioTlZNf7xfLSKi7guwN8V4loMourWPTeXHAD9J68cDF6b1ScBVaX17iufX1gbGUIyiIopk/ziwVXrvfmC7tM0Mil5XABcBX+wmrmg2pVKp0SG8h2PK14xxOaY8jilfOna21PF+dSx9OdJPRMS7kj4PXCtpKcWg1J9L708FfiTpMYpuxpPSRnMlzaDoebUEOD79gy2RdAJwM8U/zNSIeDjt61TgKklnUHQznto3X9HMzOinx/s+SZgRcRvFOJhExPXA9V3U+Qvp4e8u3vsW8K0uyn/B8iZ+dfkTLL+5bGZmfaQ/H+890k+TaWtra3QI7+GY8jVjXI4pj2OyntR9pJ9mJikG8vc3M+sNSZ07/QwIbmGamZllcMI0MzPL4IRpZmaWwQnTzMwsgxOmmZlZBidMMzOzDE6YZmZmGZwwzczMMjhhmpmZZXDCNDMzy+CEaWZmlsEJ08zMLIMTppmZWQYnTDMzswxOmGZmZhmcMM3MzDI4YZqZmWVwwjQzM8vghGlmZpbBCdPMzCyDE6aZmVkGJ0wzM7MMTphmZmYZnDDNzMwyOGGamZllcMI0MzPLUPeEKWmQpDmSbkyvb0+v50haIOmnqVySvivpMUkPSNq5ah9HSXo0LUdWlY+T9GDa5vyq8uGSbkn1Z0kaVu/vaWY20PX3431ftDAnA3OBAIiIvSJi54jYGbgDuDbV2x/YJiLGAv8CXAjFjwF8A9gtLSVJQ9M2FwLHpm3GShqfyk8DbomIbYFfpddmZlZf/fp4X9eEKWkUMAG4GFCn9zYAPgpcn4omAtMAImI2MEzSZsDHgVkRsTgiFgO3APtL2hzYINUFmA4c3Hlf6W+l3MzM6mAgHO/r3cI8DzgFWNrFe58EfhkRr6fXI4Gnq97vSGUj0npP5QtSOcCmEfE8QEQ8B2yyal/DzMx60O+P93VLmJIOABZGxBw6nW0khwNXVm/SRb3oZtuVlZuZWR8aKMf7Neu47z2BiZImAIOBIZKmR8SRkjYCdgUOqqrfAYyuej2K4iyiA2irKh8N3JrKR3VRH+B5SZtFxHOpKb+wuyCnTJmybL2trY22trbuqpqZDUjt7e20t7evrEpLHO9XWUTUfQH2Bm6sev1F4NJOdSYAM9P6HsCdaX048CdgGLBhZT29dxewO8XZx0xgfCo/Bzg1rZ8GnNVNXGFmZrVJx86WOt6vjqWeLczOqpvPhwH/d4U3I2ZKmiBpHvAGcHQqXyTpdODuVLUcxc1ggOOAy4B1KX78m1L5WcAMSccATwKH1OH7mJlZ1/rl8V4pKw9IkmIgf38zs96QRER0dV+xX/NIP2ZmZhmcMM3MzDI4YZqZmWVwwjQzM8vghGlmZpbBCdPMzCyDE6aZmVkGJ0wzM7MMTphmZmYZnDDNzMwyOGGamZllcMI0MzPL4IRpZmaWwQnTzMwsgxOmmZlZBidMMzOzDE6YZmZmGZwwzczMMjhhmpmZZVizpwoqaxugI0rxlsraB/ggMD1Ksbju0ZmZmTWJnBbmtcCSlDgvBsYAP65rVGZmZk0mJ2EujVIsAf4R+E6U4mRg8/qGZWZm1lxyEubbKuvTwJHAz1PZWvULyczMrPnkJMzPAXsAZ0YpnlBZY4DL6xuWmZlZc1FE9FyprPWALaIUj9Q/pL4jKXK+v5mZLSeJiFCj4+hrPbYwVdZEYA5wU3q9s8q6od6BmZmZNZOcS7JTgN2BlwGiFHMoesqamZkNGDkJc0kXz1z6OqaZmQ0oPQ5cADyksj4DrKmyxgInAb+rb1hmZmbNJaeFeSKwA/AX4ErgVeBLuR8gaZCkOZJuTK8l6UxJf5Q0V9KJVeXflfSYpAck7Vy1j6MkPZqWI6vKx0l6MG1zflX5cEm3pPqzJA3LjdfMzHqnvx/vs3rJrtIHSF8GxgEbRMRESUcDe0fEZ9P7G0fEC5ImACdExARJuwPnR8QekoYDd6d9ANwL7BIRr0i6CzgxImZLmgl8NyJuknQO8GJEnCPpVGDDiDiti9jcS9bMrEbd9ZJt5uP96tDtJVmVdX6UYrLKxZlCJxGlmNjTziWNAiYAZwJfTsVfBA5fvqN4Ia1OBKalstmShknaDNgHmBVR3EeVdAuwv6TbKP5RZqftpwMHU/TmnQjsncqnAe1AXX5AMzMbGMf7ld3DnJ7+ntvFe7nNsvOAU4AhVWV/DUyS9EngBeCkiJgHjASerqrXkcpGpPWeyhekcoBNI+J5gIh4TtImmfGamVnvNP3xXqKYTCR4S2L5ZCJB1mQi3SbMKMW9afUe4M0oxbsAKmsQMLinHUs6AFgYEXMktVW9tQ7wZkTsmn7ES4CPAErLCmF0UdZTeU2mTJmybL2trY22trZu65qZDUTt7e20t7d3+36rHO8pJhMZlxLnxcANFJOJTMjZOKeX7K+AfYHX0+v1gJuBPXvYbk9gYrpWPRgYIulHFGcJ16Y61wOXpvUOYHTV9qMoziI6gLaq8tHAral8VBf1AZ6XtFk629gcWNhdkNUJ08zM3qtzY6JcLneu0hLHe2BpBEukNJlIcIHEnJXUX0FOL9l1ohSVZEmU4jWKpLlSEfH1iBgdEWOAScCtEXEExY+2b6q2N/DHtH4DxQDvSNoDWJya2bOAj6Vr3BsC/wDcHBHPAa9J2l2SgCOAn1Xt66i0flT6TDMzq4MWOt6/LdHryURyWphvqKxxlUu0KutvgTdzP6BKpfl8FnCFpJOB14BjASJipqQJkuYBbwBHp/JFkk6n6DkFUK7cEAaOAy4D1gVmRsRNVZ8xQ9IxwJPAIb2I18zMeqdZj/efA74AnBnBExI1TSbS42MlKmtX4CrgWYofYQRwWJTintwPaVZ+rMTMrHbdPVbSCiSKyUSCmicTyZ2tZG3gb9LLR6IU79T6Qc3ICdPMrHatmjAlJgL/CawTwVYSOwPlCHp8TBLy7mFCkSy3B3YBDld5+egLZmZmLWIK1ZOJBDVNJtLjPUyVNYXiZu0OwP8A+wO/YflzmmZmZq1gSQSLtWLbOPsyY04L81PAfsCzUYqjgR3JeA7TzMysyTwkUUwmIsZKXEANk4nkJMzKoAVLVNZQimdctu5drGZmZg2zSpOJ5DxWcrfK2hD4IcWoP28As1e+iZmZWXOJ4A3g62mp2Up7yaosAaOjFE+l12OAIVGKB3rzYc3GvWTNzGrXar1kJc6PYLJE15OJZPaSzWlhXk/RO5YoxRM1xGhmZtYMVsdkIlkDF/wXMC1KcVd+bK3BLUwzs9q1WguzQmJ94M0I3k2vBwGD06XanrfPSJgPA9tSDDlU2WlEKT7U66ibhBOmmVntWjhhzgb2jSgmE5HYALg5osfJRIC8S7L7r0J8ZmZmzWKdSrIEiOC1NFRelpzHSpZ2s5iZmbWSNyTGVV5I1DSZSE4LcybLb4oOphhG6I8Uz7KYmZm1ii8BM6QVJxPJ3Thr8PUVNihrF+CLUYp/qWnDJuR7mGZmtWvVe5gAEitOJhJkTyaSO/j6MlGK+4Bda93OzMysCaw4mYjInkwkZ/D1f6t6uUb6kIW1RmhmZtZIElNYhclEclqYGwDrp2Vt4OfAQb2I1czMrJGWTyYS1DyZSM33MPsT38M0M6tdq97DlLg7gl0l7gU+SjH4+h8i2D5n+x5bmCrrFpU1rOr1cJV1c68jNjMza4y7JaonE5lDDZOJ5DxWsnGUYnHlRZRikcratOYwzczMGkRCwFkRvAxcJHEzMCSC7MlEcu5hvquytlz2oWVthQcuMDOz1nN9ZSWCJ2pJlpDXwvw/wO0q67b0em+g5Z/BNDOzgSOCkLhDYrcIejWZSFanH5W1MbBHenlHlOLF3nxYs3GnHzOz2rVwp5+uJxMJsiYTyZmt5B+BWyv3MVMHoLYoxfUr3bAFOGGamdWuhRPmVl2VRzA/Z/uce5ilTp1+FgNTcnZuZmbWRFZpMpGce5hdnUUMyv0AMzOzJrFKk4nkJMx7Vda3gf9Kr08A7q0xSDMzs4aK4APVryV2Ab6Yu33OJdkTgXeAq4EZwFvAv9YQo5mZWdOJoKbJROo+NJ6kQRQjKnRExIGSLgM+ArySqnw2Ih5Idb9LMRjun1P5nFR+FMXjLQBnRMT0VD4OuIyiaT0zIian8uEUCX5LYD5waMTy+7BVsbnTj5lZjbrr9NPMx/uiLl1NJjI8go/nfO+c2Uo2Ab5KMR3Kuqk4ohQfzfkAYDIwl2IQdyiuH38lIq5b4XOkCcA2ETFW0u7AhcAe6cf4BiybJfteST+LiFdSnWMjYrakmZLGR8RNwGnALRFxjqRT0+vTMuM1M7Peafbj/QYsv4e5hGIykWtzv1zOPcwrKLL3AcAXgM8CL+TsXNIoYAJwJvDl6re6qD4RmAaQfpBhkjYD9gFmVc4YJN0C7C/pNmCDiKiMAzgdOBi4Ke1r71Q+DWinmx9QLdcx2sys+bTC8T5i1Z7wyLmHuVGU4mLg7SjFbVGKo1k+iEFPzgNO4b3dds+U9ICkb0taO5WNBJ6uqtORykak9Z7KF6RygE0j4nmAiHgO2KS7ACO8ePHixUstS6se7yVukRhW9Xp4GlM2S04L8+309zmVdQDwTFWg3ZJ0ALAwIuZIaqt662sR8Vz64X4AnAqcTnEW0vlMJLoo66m8JlOmTFm23tbWRltbW7d1zcwGovb2dtrb27t9v1WO98DGESwfVyBYJJE9mUhOwjwzje7zb8AFwBDg5Izt9gQmpmvVg4EhkqZHxJFFoPF2uiFcuQnbAYyu2n4UxVlEB9BWVT4auDWVj+qiPsDzkjZL/1CbAwu7C7I6YZqZ2Xt1bkyUy+XOVVrieA+8K7FlBE/CspF/8icTiYi6LxTXl29M65unvwK+A3wrvZ5A0fMJiku+d6b14cCfgGHAhpX19N5dwO5pXzOB8an8HODUtH4acFY3cYWZmdUmHTtb6nifwh4P8RTEj9LyFMT47up3XnJamKuDWN58vlzSxqlsDvB1gIiYKWmCpHkUg+IencoXSToduDttX47lXYaPo+hmvC7Fj39TKj8LmCHpGIpBdg+p55czM7NlmvZ4H8FNEuNY3g/n5AiyJxOp+3OYzczPYZqZ1a6FB18vJhNJ9zFTB6C2CLImE3HCHMDf38ysN1o4YT4QwY6dyu6PYKec7XMGLhgM/BOwVVX9iFJ8s8ZYzczMGmmVJhPJuYf5M2AxxYDrb+Xu2MzMrMncK9HryURyEubIKEXWOHtmZmZN7ETgPyhGrxMwixomE+nxHqbK+gHwvSjF71chyKbke5hmZrVr1XuYqyonYT4MbAM8AfwlFUeU4kN1jq3unDDNzGrXqglTouvJRIKsyURyLsnuX9lp5TNritDMzKw59HoyEch8rERl7QTsRZE0b49SMZ9Zq3ML08ysdi3cwrwvgl0kfh/Bh1LZryP4SM72Pc5WorImA5cDGwObAperrJNWJWgzM7MGWD6ZiDhAYhcyJhOpyLkkeyywe5TiDQCVdRZwJ/DdWiM1MzNroDPT6D61TiYC5CVMWHE09/yR3c3MzJpEBDem1cWsOCtKlpyEeSkwW2VdR9Hh52Dgklo/yMzMrJXldvoZB3w4vfx1lGJOXaPqI+70Y2ZWu1bt9LOquk2YKmtIlOJVlTW8UpT+FhNJlmJRH8RXV06YZma1G6gJc2WXZK8EPgHcx/JnMKuNqUtEZmZmdSDR9WQiQdZkIp7eawB/fzOz3mjVFqbEzSyfTOTdSnkE5+ZsnzO916+iFPv2VGZmZtbkRkbQ68lEuk2YKmtdYD1g46r7mFA8tzKitx9oZmbWIL+T+FAEvZpMZGUtzC8AkymSY/V8Ya+xfC4xMzOzVrEXcLTUaTKRNExeT3JmKzkpStEvR/XxPUwzs9q18D3Mrboqj2B+1vaZz2F+gGI6lMHLPqAU07MibGJOmGZmtWu1hCkxJIJXJYZ39X4EWY9J5nT6mQLsDewA/A/FdF+/AVo+YZqZ2YCwWh6T7HG2EuBTwH7As1GKo4EdqWppmpmZNbMIPpH+bhXBmM5L7n5yxpJ9M0rxrspaorKGAguBrXsZt5mZWcNIbAiMpfoWY/DrnG1zEubdKmtD4IfAPcAbwOxexGlmZtYwEp8HTgJGA3OAPYA7gI9mbV9LpxeVNQYYEqV4oPZQm487/ZiZ1a7VOv1USDwE7ArcEcFOEu8HvhnBoTnbr2zggnF0fXMUlbVLlOK+3gRsZmbWIG9F8KZUjCsbwSMSf5O78couyZ5LkTDXBcbBspERPkRxSfbD3WxnZmbWjDrSPczrgVskXgYW5G7cbS/ZKEVblGIfYD6wS5RiXJRiHLAz8HjuB0gaJGmOpBs7lV8g6bWq1+tIulrSY5LulLRl1XtfS+WPSPpYVfn4VPaYpFOrysdImi3pUUlXSVorN14zM+udZj/eR3BwBC9HMAX4D+Bi4ODc75fzWMl2UYoHl31gKR4Cdsr9AIrh9eZSdXlX0t8CQ1nxku8xwEsRMRY4Dzg71d0eOIxi4ITxwPdVGAR8L5VtDxwuabu0r7OBcyNiW+DltG8zM6uvpj3eSwxK9zABiKA9ghsieDv3y+UkzIdV1sUqqy0tP6T4QXokaRQwgSKLK5UNAs4BvlopSyYC09L6tUBlNpSDgCsj4p2ImA/MA3YHdgPmRcT8iHgHuAo4SJKAfYBr0vbTqOEMwszMatfsx/sI3gUekNiyq/dz5DxWcjRwHMWZA8CvgQsz938ecArFDCcVJwA/i4jniu+6zEjgaYCIWCLpFUkbUQz+fmdVvY5Ul0r9qvLdgeHA4ohYmsoXVNU3M7P6aIXj/QjgDxJ3UTwimUJgYs4X7DFhRineBL6dlmySDgAWRsQcSW2pbATFyEFt6vTrseLZx7KPX0l5V63jldXv0pQpU5att7W10dbW1l1VM7MBqb29nfb29m7fb5XjPTCli22yny1c2WMlP4lSHKKyHuzi7YhS9DQdyp7AREkTKEZUGAI8BLxF0cwGWE/So+nacwewBfCMpDWBoRGxSFIHxUOmFaMoziLUqXw0sCAiXpQ0TNIa6axjFPBMd0FWJ0wzM3uvzo2JcrncuUpLHO+BT0Tw1eoCibOB23r6DWDl9zArl2AP7GLpsfkaEV+PiNERMQaYBNwaEcMjYkREjEnlf04/HsANwFFp/VPAr6rKJ0laW9IYiiGN7qIYdWispK0krU1xo/iGtM3/Aoek9aMouhCbmVkdtNDx/h+6KJuQ+z27bWFGKZ5Jf+fn7qwHXTV7q8umAj+S9BjwEsWPTkTMlTSDoqPREuD4NDzPEkknADcDg4CpEfFw2tepwFWSzqAYnX7qavoOZmbWs6Y63kscBxwP/LVE9VXTDYDf5n6pbofGU1mv0/213YhSDOnmvZbhofHMzGrXakPjSQwFNgTOokiwldhfi+Cl7P0M5IThhGlmVrtWS5irS85jJQCorE2ong6lFE/VJSIzM7Mm1GPCVFkTKcaVHUExF+aWwMPADvUNzczMrHnkjPRzBvB3wKNRijEUIzL8rq5RmZmZNZmchPlOlOJFYA2VNShK8b/UNpasmZlZy8u5h/myytoAuB24QmUtBN6pb1hmZmbNJaeFeRDwZ+Bk4CaKURsOrGdQZmZmzSanhfkFYEaUogO4rL7hmJmZNaechDkEuFllvQxcCVwTpXi+vmGZmZk1l+yBC1TWjsChFOP+dUQp9u1hk6bngQvMzGo3UAcuyLmHWbEQeI5i3L+N6xOOmZlZc8oZuOA4ipHhN6GY1frYKMXcegdmZmbWTHLuYW4JfClKcX+9gzEzM2tWHnx9AH9/M7Pe8D1MMzMz65YTppmZWQYnTDMzswxOmGZmZhmcMM3MzDI4YZqZmWVwwjQzM8vghGlmZpbBCdPMzCyDE6aZmVkGJ0wzM7MMTphmZmYZnDDNzMwyOGGamZllqHvClDRI0hxJN6bXUyXdL+kBST+R9FepfB1JV0t6TNKdkras2sfXUvkjkj5WVT4+lT0m6dSq8jGSZkt6VNJVktaq9/c0Mxvo+vvxvi9amJOBuUBl4skvRcROEbEj8BRwQio/BngpIsYC5wFnA0jaHjgM2B4YD3xfhUHA91LZ9sDhkrZL+zobODcitgVeTvs2M7P66tfH+7omTEmjgAnAxYAAIuK19J6A9YClqfpEYFpavxbYN60fBFwZEe9ExHxgHrA7sBswLyLmR8Q7wFXAQWm/+wDXpO2nAQfX6zuamdnAON7Xu4V5HnAKy38kACRdCjwLbAtckIpHAk8DRMQS4BVJGwEjgI6qzTtS3RGV+p3KhwOLI6LymQtSuZmZ1U+/P97XLWFKOgBYGBFzSGcbFRFxNMUP8AgwqbJJF7uJ1VhuZmZ1MFCO92vWa8fAnsBESROAwcAQSdMj4kiAiFgqaQbwFeAyijOGLYBnJK0JDI2IRZI6gNFV+x1FcRahTuWjgQUR8aKkYZLWSGcdo4BnugtyypQpy9bb2tpoa2tbtW9tZtbPtLe3097evrIqLXG8X1WKqH/jS9LewFci4kBJ20TEvHTt+T+BpRHxVUnHAx+MiOMkTQIOjohJ6SbwjymuYY8EfglsAwwC/khx7fsZ4C7g8Ih4OP3DXBsRV0u6CLg/Ii7qIq7oi+9vZtafSCIiumrdNe3xfnWoZwuzmoBIP9plkoaksvuB41KdqcCPJD0GvERqukfE3PSDzAWWAMenLLdE0gnAzRQ/5tSIeDjt61TgKklnAPelfZuZWf312+N9n7Qwm5VbmGZmtVtZC7M/80g/ZmZmGZwwzczMMjhhmpmZZXDCNDMzy+CEaWZmlsEJ08zMLIMTppmZWQYnTDMzswxOmGZmZhmcMM3MzDI4YZqZmWVwwjQzM8vghGlmZpbBCdPMzCyDE6aZmVkGJ0wzM7MMTphmZmYZnDDNzMwyOGGamZllcMI0MzPL4IRpZmaWwQnTzMwsgxOmmZlZBidMMzOzDE6YZmZmGZwwzczMMjhhmpmZZXDCNDMzy1D3hClpkKQ5km5Mr6+Q9IikByVNlbRmVd3vSnpM0gOSdq4qP0rSo2k5sqp8XNrPY5LOryofLumWVH+WpGH1/p5mZgNdfz/e90ULczIwF4j0+vKIeH9EfBCH1kBbAAAH8ElEQVRYFzgWQNIEYJuIGAv8C3BhKh8OfAPYLS0lSUPTvi4Ejk3bjJU0PpWfBtwSEdsCv0qvW0J7e3ujQ3gPx5SvGeNyTHkc02rRr4/3dU2YkkYBE4CLAQFExC+qqtwNjEzrBwHTUp3ZwDBJmwEfB2ZFxOKIWAzcAuwvaXNgg1QXYDpwcFqfWNlX+lspb3rN+D+IY8rXjHE5pjyOadUMhON9vVuY5wGnAEs7vyFpLeCfgZtS0Qjg6aoqHRQ/7oi03lP5Apb/Y2waEc8DRMRzwCar+kXMzGyl+v3xvm4JU9IBwMKImEM62+jk+8BtEfHbyiZd1Itutl1ZuZmZ9aEBc7yPiLoswLcoziCeAJ4F3gCmp/dKwHWd6l8ETKp6/QiwKTAJuKiq/L+Bw4DNgIeryg8HLqzadrO0vjnwSDcxhhcvXrx4qX1pteP9aslr9dpxpx9nb+DGtH4s8FtgcKc6E4CZaX0P4M60Phz4EzAM2LCynt67C9id4uxjJjA+lZ8DnJrWTwPO6ovv6cWLFy8DfenPx/tlXXzrTBRnJVD0dJoP3CEJ4NqIOCMiZkqaIGkexdnJ0QARsUjS6RQ3jAHKUdwMBjgOuIyi99XMiKhcHz8LmCHpGOBJ4JB6fjkzM1um3x7vlbKymZmZrcSAHOlH0vj0MO1jkk5tdDwAki6R9LykBxsdS4Wk0ZL+V9JcSQ9JOqkJYhosabak+1NMUxodU0Xnh7YbTdJ8Sb9PMd3V6HgAJA2TdI2kh9N/V3s0QUx/k36jyvJKk/y3fnL6b/xBST+WtE4TxDQ5xfOQpMmNjqevDbgWpqRBwB+B/Si6Jt8NHB4RDzc4rr2A1ylulH+wkbFUpOeiNouI+yWtD9wLHNwEv9V6EfHnNGrIb4DJsfz5rEbG9WVgHMXzYhObIJ4ngHERsajRsVRImkbRW/KS9O/3VxHxSqPjqpC0BsVxYbeIeLqn+nWMYyRwO7BdRPxF0tUUlyGn9bBpPWP6AHAlsCvwDsUjIsdFxLxGxdTXBmILczdgXkTMj4h3gKsoHqJtqIi4HXi50XFUi4jnIuL+tP468DDF81ANFRF/TqtrA2vRxXNffa2rh7abRNPEImkIsFdEXAIQEUuaKVkm+wGPNzJZVlkTWC+dWKxHkcgb6f0UnXPeioh3gduATzY4pj41EBPmSLp+YNZWQtJWwM5AM7Tk1pB0P/A8xaggd/e0TR/o9qHtBgpglqR7JH2+0cEAWwMvSLpU0n2SfihpvUYH1ckk4MeNDiIiFgDnAk8BzwCLI+KXjY2Kh4CPpLFb1wM+AYxqcEx9aiAmzK4MrOvSNUqXY6+huPT5eqPjiYilEbETxf+su0vaoZHxZDy03Sh7RsQ4YH/gX9Nl/0ZaE9gF+H5E7ELRO7JpxnmWtDZwIPCTJohlQ4oh37aiuKqzvqTPNDKmiHgEOJtiuLpfAPcD7zYypr42EBNmBzC66vVoGn+po2mlIa2upRhE+fpGx1MtXc5rB8b3ULXe9gQmpnuGVwIflTS9wTERxTBhRMQLwE8pbkc0UgfQUXVF4BqKBNos9gfuTb9Xo+0HPBERL0XEEuA6iv/OGioiLomIcRGxN8UtpEcbHVNfGogJ8x6Kke63SmeUhwE3NDimpqTiwampwNyI+E6j4wGQ9L7K9D2S1qU4sDS0E1JEfD0iRkfEGIpLerdGxJE9bVdPktaTtEFa/yvgY0BDe2CnBP60pG1T0X7AHxoYUmeHU5zwNIMngT0krZv+P9yPYhaQhpK0Sfq7BcX9y2b5vfpEXw1c0DQiYomkE4CbgUHA1Eb3+gSQdCXFCBkbSXoa+EZEXNrgsP6eYsDk30uak8q+VvXAcCNsDkxLvZ3XAK6OiJkNjKcrzXCJf1Pgp+lh8TWBKyJiVmNDAuBE4Ip0svo46YH1Rkv35PYDmuFeLxFxl6RrgPuAJenvDxobFQDXSNqIopfs8U3YaauuBtxjJWZmZr0xEC/JmpmZ1cwJ08zMLIMTppmZWQYnTDMzswxOmGZmZhmcMM3MzDI4YZq1EEltzTJ9mNlA44RpZmaWwQnTrA4k/XOa6HqOpIvS5NKvS/p/ku6V9EtJ70t1d5J0p6QHJF1XNfTfNqne/WmbrSlGEVpf0k/SJMyXV33mWZL+kPbzn4355mb9lxOm2WomaTvgUIrZQnammNHhMxRzGt6bZhC5DSilTaYDp0TEjhTjvVbKrwAuSDOz/B3wLMVsKDsDk4Htga0l/b2k4RSTe++Q9nN6H3xVswHFCdNs9dsXGAfck8bg/SgwhmKuzKtTncuBD6dJlYemCcQBplHMObg+MCIifgYQEW9HxJupzl0R8UwU41reD2wJvAK8JeliSZ8EKnXNbDVxwjSrj2kRsXNatouIcqf3RdeDtKvT3678pWr9XWCtiHiXYvqua4GDgUYOkG/WLzlhmq1+vwI+JWljgDRD/ZYU/78dkup8Grg9Il4FXpb04VR+BNAeEa8BHZIOSvtYJ01n1qU0hdewiPgFcDKwUz2+mNlANuCm9zKrt4h4WNK/A7MkrQG8DZwAvAHsIOkeYDHFXKwARwEXpSmmqqe8OgL4b0nfTPs4lKJV2rllGsAGwM8kDaZonX6pXt/PbKDy9F5mfUTSaxGxQaPjMLPe8SVZs77js1OzFuYWppmZWQa3MM3MzDI4YZqZmWVwwjQzM8vghGlmZpbBCdPMzCyDE6aZmVmG/w/kHYQs6t+iJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113ea5d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEZCAYAAAAkDXpUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAHnFJREFUeJzt3XmYXVWZ7/HvjyRMnYTZgRAJk8gkIILRBlIKty/GNmKL2ogKubZ6VRQHbBqveuqAaIva0qCNNBoEh0CLE1xRwEhBbG0ggYQpIKEJJIBBMcygIbz9x14Fh0MN+xS1a59F/T7Ps586e35PJVlv1tprr6WIwMzMLAfr1R2AmZlZWU5aZmaWDSctMzPLhpOWmZllw0nLzMyy4aRlZmbZcNIyK0nSRpIulHS/pPPG+N43SDpwLO9p1o0m1h2AWackrQDeExELxvjWhwEvADaPiCeruomkbwMrI+Iz/dsiYveq7meWE9e0LEeRlrG2LfC7KhOWmQ3NScueNyRtIOkUSXel5auS1k/7tpT0/yWtkXSfpCtazjtO0ipJD0q6WdLrBrh2E/gM8HZJD0n6P5J6JX2n5ZgZkp6UtF5a75N0gqRfp2tfLGmLluP3l/SbFNOdko6U9F7gHcA/pvv8NB27QtJBJb5nT/ouH5e0WtLdko5quedsSTemeFZJ+sSo/iGYVcxJy55P/h+wH7BnWvYDPp32fQJYCWxJ0cR3PICknYEPAa+MiKnA3wAr2i8cEQ3g88C5ETElIuZRrrZ3OHBUuuf6wLHpvi8BLgL+NcW0F7AkIs4Evgd8Md3nTf0htNxvqO8J8EJgKrA18B7g65I2Sfu+BbwvfdfdgF+V+A5mXcNJy55P3gGcEBF/jIg/Ak3gXWnfX4AXAzMiYl1E/Gfavg7YANhN0qSIuDMi/nuQ6ystretDCeCsiFgeEY8D/0GRnACOAC6NiPNSPH+KiKUlrz3U9wRYm/avi4ifAw8DO6d9f0nfdWpEPBAR1w7zHcy6ipOWPZ9sDdzRsn5n2gbwJWA5cImk2yQdBxARy4GPAr3AaknzJb14FGP6fcvnx4DJ6fN0YLDkOJyhvifAfW3P3R5tue9bgNnAitR8OXOEMZjVwknLnk/uBma0rL8kbSMiHo6IYyNiB+CNwMf7n11FxPyIOICio0UAXxzk+u3NgQ8DG7esv6iDWO8Edih5n3aDfs/hRMSiiDgU2Ar4CUXtzywbTlqWq/UlbdiyTATmA59OnS62BD4LfAdA0t9K2lGSgIcomgWfkPRSSa+TtAHwZ+Bx4IlB7tneZLcEOFDS9PTM6PgS5/T7PnCwpLdKmihpC0l7pn2rge2H+O6Dfs+hSJok6QhJm0TEOorfw2Df1awrOWlZri6iaPbqXz4LfA5YBFyXlkVpG8COwKUUBfVvgK9HxBUUz7O+APwBuIeiU8SnBrnnM7raR8QvgfPSva4GLuTZtaRo+xzp3Dspmuk+AdwHXAu8PB33LWDX1KvwRwPEMdT3bL9nu3cCt0t6AHhfWjfLhqqcBFLSPOANwL0Rsccgx5wKvJ6i4Dmq/8Fw6l31TWAbin+EsyPijoGuYWZmI5dTWV11Tess4JDBdkqaDewYETtR/K/v9Jbd51B0+90V2Be4t8pAzczGsWzK6kqHcYqIhZJmDHHIHODsdOyVkjaV9EJgC2BC/zA9EfFolXGamY1nOZXVdT/Tmkbxwme/VRRVzJ2A+yX9UNI1kk7uH2XAzMzGXNeU1XUngvaXNQGeBCYBB1A8pN6XoifVUWMamZmZ9euasrruUd5XUbxk2W8bivdN1geujYgVAJJ+AswE5rWeLKmOQVPNzLIXEcON6NLqOZXVo6numtYFwLsB0pv590fEaoouvJuld1AADgJuHOgCEdFVS6PRqD2GXOJyTI5pPMTVjTHVUVaPlkprWpLmA7OALSWtBBoU1Uki4oyIuCiNOr0ceASYm/atk3QssCC9DLoIOLPKWM3Mxqucyuqqew8eXuKYowfZ/kuKEazNzKxCOZXVdTcPPu/09PTUHcKAujEux1SOYyqvG+PqxphyVumIGFWTFDnHb2ZWB0lEZx0xuoZrWmZmlg0nLTMzy4aTlpmZZcNJy8zMsuGkZWZm2XDSMjOzbDhpmZlZNpy0zMwsG05aZmaWDSctMzPLhpOWmZllw0nLzMyy4aRlZmbZcNIyM7NsOGmZmVk2nLTMzCwbTlpmZpYNJy0zM8uGk5aZmWXDScvMzLLhpGVmZtlw0jIzs2w4aZmZWTactMzMLBtOWmZmlg0nLTMzy0alSUvSPEmrJV0/xDGnSrpV0lJJe7ftmyrpLkmnVRmnmdl4llNZXXVN6yzgkMF2SpoN7BgROwHvA05vO+REoK+y6MzMDDIqqytNWhGxEFgzxCFzgLPTsVcCm0p6IYCkfYAXAJdUGaOZ2XiXU1ld9zOtacDKlvVVwDRJ6wFfBo6tJSozM2vVNWV13UlLaWn3QeCiiLhrkP1mZjZ2uqasnjgWNxnCKmB6y/o2wN3ATOAASR8EJgPrS3ooIj7VfoHe3t6nPvf09NDT01NlvGZm2enr66Ovr++5XOI5l9WjRRFR1bWLG0gzgAsjYo8B9s0Gjo6I2ZJmAqdExMy2Y44EXhkRHx7g/Kg6fjOz5xtJRITats2gorJ6NFVa05I0H5gFbClpJdAAJgFExBkRcZGk2ZKWA48Acwe5lDOTmVlFciqrK69pVck1LTOzzg1U08pF3R0xzMzMSnPSMjOzbDhpmZlZNpy0zMwsG05aZmaWDSctMzPLhpOWmZllw0nLzMyy4aRlZmbZcNIyM7NsOGmZmVk2nLTMzCwbTlpmZpYNJy0zM8uGk5aZmWXDScvMzLLhpGVmZtlw0jIzs2w4aZmZWTactMzMLBtOWmZmlg0nLTMzy4aTlpmZZcNJy8zMsuGkZWZm2XDSMjOzbDhpmZlZNpy0zMwsG05aZmaWjUqTlqR5klZLun6IY06VdKukpZL2Ttv2kvQbSTek7W+rMk4zs/Esp7K66prWWcAhg+2UNBvYMSJ2At4HnJ52PQK8KyJ2T+efImlqxbGamY1X2ZTVlSatiFgIrBnikDnA2enYK4FNJb0wIm6NiNvS9nuAe4GtqozVzGy8yqmsrvuZ1jRgZcv6KmCb1gMk7QdM6v/FmJnZmOuasnpilRcvQWlp9eRTO6UXA+cA7x7sAr29vU997unpoaenZ1QDNDPLXV9fH319fc/lEs+5rB4tiohqbyDNAC6MiD0G2PcNoC8izk3rNwOzImJ1ahe9DPh8RPxwkGtH1fGbmT3fSCIi1LZtBhWV1aNp2OZBNTVZTU1In3dWU3PU1KRRuv8FpMwsaSZwf/olrA/8GDhnLH4JZmY2pK4pq8s0D14B7K+mNgN+CSwC3g4cMdyJkuYDs4AtJa0EGsAkgIg4IyIukjRb0nKKXihz06lvAw4ANpd0VNp2ZERcV/qbmZlZKTmV1cM2D6qpa6MRe6upDwMbRSNOVlNLohF7VRVUWW4eNDPr3EDNg7ko1XtQTb2aomb1s7RpQmURmZmZDaJM0voocDzw42jEjWpqB4qHbmZmZmOqo96Damo9YHI04sHqQirPzYNmZp3LuXmwzDOt+cD7gXXAYmBT4F+iESdXH97QnLTMzDqXc9Iq0zy4a6pZHQpcBEwH3lVpVGZmZgMok7QmpveyDgUuiEasBVy9MTOzMVcmaZ0BrAAmA1eoqRnAA9WFZGZmNrCOh3FSUwImphpXrfxMy8ysczk/0xp2RAw1tSnF29EHpk19wAm4tmVmZmOsTPPgPOBB4K0UQ3Y8RDFhmJmZ2ZgqM/bgDtGIv2tZ71VTS6sKyMzMnr8kJgOPRbBOYmdgZ+DnEZR65FSmpvWYmjrgqRs2tT/w6IiiNTOz8e4KYAOJaRSDsM8Fvl325DI1rf8LnKOmNknra4AjOwzSzMwMQBE8KvEe4LQITpZYUvbkYZNWNGIJ8HI1NTWtd8UQTmZmlieJ/kHY35M2lR6EfdCkpaY+0bIaLdsFRDTiXzqM08zM7OlB2IMbJToahH2omtYUPPKFmZmNogguBy4HkFgP+EMEHyl7fscvF3cTv1xsZta5Ol8ulhh4EPag1CDspSaBNDMzGyW7RjDiQdidtMzMbCxNlHh6EPbi/azSTWZOWmZmNpaeOQi7mEEHwwKWmQRyQ+AtwAye7rgR0YgTOo91dPmZlplZ57ppwFyJYhD2URwR46fAHGAt8HBaHhlxhGZmNm5JbCrxVYnFEouBLwMblz2/zIgY06IR/3vEEZqZmT1tHnA9xSDsouiEcRbwd0Od1K9MTes3aurlIw7PzMzsaTtE0IjgvyO4LYJeYIeyJ5epaR0AzFVTtwN/TtsiGuFEZmZmnXpM4oAIFgJIdDQIe5mk9fr0s7/HQ1c8vDMzsywVg7CLEQ3CXmpEDDW1F0WNK4CF0YiumE/LvQfNzDrXDb0HJYpB2IsXjcufV6LL+zHAe4EfUdSyDgXOjEacOrJQR4+TlplZ5+pIWhIDDsJOkVciglKDsJfpiPEPwKuiEZ+NRnwGmEmRxEoEqXmSVku6fohjTpV0q6SlkvZu2X6kpN+l5d1l7mdmZp0bo7J6CsULxZPT5ykt26aUjbXMMy2AJwf5PJyzgNOAcwbaKWk2sGNE7CTpVcDpwExJmwOfBfZJhy6WdEFE3N/Bvc3MrJzKy+rUS/A5K5O0zgKuVFOtzYPzylw8IhZKmjHEIXOAs9OxV0raVNKLgNcCl/R/cUmXAocA55a5r5mZlZdTWT1s82Ca7HEuRQ+PPwFHRSO+Okr3nwasbFlflbZtnT63bzczs7HXNWX1UDMXT41GPKimNgdupxjgECDU1ObRiD+Nwv3Fs7vQxwDb+rc/+wLugG9mNoy+tIzYcy6rR8tQzYPzgTcA1wwSxHajcP9VFHOp9NsGuCtt72nZPh341UAXcOdBM7Ph9NBapErNTi/wnMvqp+/NwIOwB6UGYa985uLUTnphROwxwL7ZwNERMVvSTOCUiOh/uLcIeAVFJl8MvKL94Z67vJuZdW6gLu9VltXPvBYXA/enY9f1b4/gK2ViH7YjhppaEI04aLhtAwen+cAsYEtJK4EGMKkIMM6IiIskzZa0nGLk+Llp358knQhcnS7VdM9BM7NqjHFZPS2CEQ/CPmhNS01tRDFc/GU8s/o3Ffh5NGKXkd50tLimZWbWuTpHxJD4d+BrEVw3kvOHqmm9HziGonfI4pbtDwFfH8nNzMxs3CsGYRfPHIQ9KDUIe5lhnD7SDUM2DcQ1LTOzztVc05ox0PaIp3qoD31+yQFzdwd2BTZ86gaNGPDN6bHkpGVm1rmaxh6cGsGDEpsPtD+CUq9RlemI0UvxgG434GcUU5X8mkGG+zAzMxvAqLxGVWbA3MOAg4F7ohFzgT1pqXGZmZkNJ4I3pJ8zItiufSl7nTJjDz4WjVinpp5QU5sA9wLbjzBuMzMb5yQ2A3ai9ZFTcEWZc8skravV1GbAmRQvkT0CXDmCOM3MbJyTeC/wEYrRM66lmO7qt8DrSp3fSUcGNbUdMNUzF5uZ5avm3oM3APsCv41gL4mXASdE8LYy5w81YO4+DDZIbVOviEZcM5KAzcxsXHs8gsekYhzCCG6W2LnsyUM1D36FImltRDHBV//byy+naB7cf6QRm5nZuLUqPdP6CXCpxBqKwXdLKfNy8bnASdGI69P67sAnoxFHjjzm0eHmQTOzztXZPPjMOOihGBrwFxH8pcw5Zbq879KfsACiETcAe40oQjMzG7ckJqRnWgBE0BfBBWUTFpTrPbhMTX0T+G5aPwK4qbNQzcxsvItgncRSiW0juGMk1yiTtOYCH6AYPBfgCuD0kdzMzMzGva2BGyWuoniFCooBc+eUObnySSCr5GdaZmadq7nL+yyKCSNbRQSXlzl/qC7vP4hGvFVNXT/A7ohGlBpG3szMrMUbIvjH1g0SX4TnmLR4ujnwjSMMzMzMrN3/GmDbbOC4Mie7edDMbJypaWqSDwAfBHYAbmvZNQX4zwiOKHWdwQp9NfUwg4yIQdE8OLV8uNVw0jIz61xNSWsTYDPgnylqVf33fyiC+0pfJ+dC30nLzKxz3fJy8UiU6fIOgJp6Ac+cufjOSiIyMzMbRJmZi+dQjEO4NcVcWtsCyyhmMjYzMxszZYZx+hzwauB30YjtgIOA31QalZmZ2QDKJK210Yg/AuupqQnRiMvw2INmZlaDMs+01qipKcBC4Htq6l5gbbVhmZmZPVuZmtabgEeBjwG/AJbjF47NzKwGZWpa7wf+IxqxCvh2teGYmZkNrkzSmgpcrKbWAPOB86MRq6sNy8zM7NmGbR6MRvRGI3YDPkTR7f0KNbWgzMUlHSLpZkm3SnrWuFKStpW0QNJSSZdJmtay72RJN0i6SdK/dvCdzMysQ7mU12WeafW7F/g9cB+w1XAHS5oAfA04BNgVOFzSLm2HfRn4dkTsCZwAfCGd+xrgNcAewO7AvpJmdRCrmZmVlFN5PWzSUlMfUFN9wAKKZPUPJacl2Q9YHhErImItcC5Fp45Wu6TrAvS17A+K0Tc2ADYCJlEkTDMzG33ZlNdlnmltC3w0GrGkw2tPA1a2rK8CXtV2zFLgMOBU4M3AFEmbRcRvJfUB91AMqnhaRNzS4f3NzKycbMrrYZNWNOKfRvF+7aPbHgt8TdJRwBXAXcATknYEXkbxixRwqaRLImJh+wV7e3uf+tzT00NPT88ohmtmlr++vj76+vo6PW3Uy+vRUNko75JmAr0RcUhaPx54MiK+OMjxk4FlETFd0ieBDSLic2nfZ4DHI+JLbed4lHczsw61j/I+FuX1aOmkI0anFgE7SZohaX3g7cAFrQdI2kJSfwzHA99Kn+8AZkmaIGkSMAu4qcJYzczGs2zK68qSVkQ8ARwNXEzxBc6LiGWSmpL6R9R4LXCzpFsoOnmclLafTzGz5fXAEmBJRPysqljNzMaznMprTwJpZjbO5DwJZJXNg2ZmZqPKScvMzLLhpGVmZtlw0jIzs2w4aZmZWTactMzMLBtOWmZmlg0nLTMzy4aTlpmZZcNJy8zMsuGkZWZm2XDSMjOzbDhpmZlZNpy0zMwsG05aZmaWDSctMzPLhpOWmZllw0nLzMyy4aRlZmbZcNIyM7NsOGmZmVk2nLTMzCwbTlpmZpYNJy0zM8uGk5aZmWXDScvMzLLhpGVmZtlw0jIzs2xUmrQkHSLpZkm3SjpugP3bSlogaamkyyRNa9n3EkmXSLpJ0o2Stq0yVjOz8SyX8loRUc2FpQnALcDBwF3A1cDhEbGs5ZgfABdExHckvRaYGxHvTvv6gBMjYoGkjYGIiMfa7hFVxW9m9nwliYhQy3rl5fVoqbKmtR+wPCJWRMRa4FzgTW3H7AIsSJ/7+vdL2hWYEBELACLi0ap+AWZmlk95XWXSmgasbFlflba1Wgoclj6/GZgiaTPgpcD9kn4o6RpJJ0vy8zczs2pkU16PdSJob8s7Fpgl6RrgQIpq6RPAROAA4BPAvsD2wFFjF6aZ2bjXleX1xKouTJGpp7esT6f4kk+JiHuAtwBImgy8JSIekrQKuDYiVqR9PwFmAvPab9Lb2/vU556eHnp6ekbzO5iZZa+vr4++vr6hDhmT8no0VNkRYyLFg72DgLuBq3j2g70tgDUR8aSkk4C1EdGbHgouBg6OiD9KOgu4KiJOb7uHO2KYmXVogI4YlZfXo6Wy5sGIeAI4GrgYuAk4LyKWSWpKemM67LXAzZJuAbYCTkrnrqOoii6QdB1FNfXMqmI1MxvPciqvK6tpjQXXtMzMOtde08qJe+SZmVk2nLTMzCwbTlpmZpYNJy0zM8uGk5aZmWXDScvMzLLhpGVmZtlw0jIzs2w4aZmZWTactMzMLBtOWmZmlg0nLTMzy4aTlpmZZcNJy8zMsuGkZWZm2XDSMjOzbDhpmZlZNpy0zMwsG05aZmaWDSctMzPLhpOWmZllw0nLzMyy4aRlZmbZcNIyM7NsOGmZmVk2nLTMzCwbTlpmZpYNJy0zM8tGpUlL0iGSbpZ0q6TjBti/raQFkpZKukzStLb9UyXdJem0KuM0MxvvcimvK0takiYAXwMOAXYFDpe0S9thXwa+HRF7AicAX2jbfyLQV1WMVejr66s7hAF1Y1yOqRzHVF43xtWNMbXLqbyusqa1H7A8IlZExFrgXOBNbcfsAixIn/ta90vaB3gBcEmFMY66bv0L2o1xOaZyHFN53RhXN8Y0gGzK6yqT1jRgZcv6qrSt1VLgsPT5zcAUSZtJWo8iqx9bYXxmZlbIprwe644Y0bZ+LDBL0jXAgcBdwDrgg8BFEXEXoLEN0czM6NbyOiIqWYCZwC9a1o8Hjhvi+MnAyvT5u8AdwO3AH4AHgM8PcE548eLFi5fOl7Eur0drUbrpqJM0EbgFOAi4G7gKODwilrUcswWwJiKelHQSsDYietuucyTwyoj4cCWBmpmNczmV15U1D0bEE8DRwMXATcB5EbFMUlPSG9NhrwVulnQLsBVw0mCXqypOM7PxLqfyurKalpmZ2WjLdkSM4V6EqyGeeZJWS7q+7lj6SZqeXgK8SdINkj7SBTFtKOlKSUtSTL11x9RP0gRJ10q6sO5Y+klaIem6FNdVdccDIGlTSedLWpb+bs2sOZ6d0++nf3mgS/6ufyz9Hb9e0vclbdAFMR2T4rlB0jF1xzMSWda00otwtwAHU/RguZq29tcaYjoAeBg4JyL2qCuOVpJeBLwoIpZImgwsBg6t8/eU4to4Ih5N7ei/Bo6JiCvrjCnF9XFgH2BKRMypOx4ASbcD+0TEn+qOpZ+ks4HLI2Je+jP8q4h4oO64AFL367uA/SJi5XDHVxjHNGAhsEtE/FnSeRQ97M6uMabdgfnAvsBa4BfAByJieV0xjUSuNa0yL8KNqYhYCKypM4Z2EfH7iFiSPj8MLAO2rjcqiIhH08f1gUnAkzWGA4CkbYDZwDfpvtcsuiYeSVOBAyJiHhTPQrolYSUHA7fVmbBaTAQ2Tol9Y4pkWqeXAf8VEY9HxDrgcor3rbKSa9Iq8yKctZA0A9gb6IYazXqSlgCrgUsi4uq6YwK+CnySLkigbQK4RNIiSe+tOxhge+APks6SdI2kMyVtXHdQLf4e+H7dQaR3lr4C3EnRG+/+iPhlvVFxA3CgpM3Tn9kbgG1qjqljuSatgeTXzjlGUtPg+RTNcA/XHU9EPBkRe1H8g3mVpN3qjEfS3wL3RsS1dFGtJnlNROwDvB74UGqGrtNE4BXAv0XEK4BHgH+qN6SCpPWBNwI/6IJYNgPmADMoWjcmSzqizpgi4mbgi8ClwM+BJRQvB2cl16S1Cpjesj6d+qveXUnSJOCHwHcj4id1x9MqNSv1UQzSWafXAHPS86P5wOsknVNzTEDRxJt+/gH4MUXTeJ1WAataasfnUySxbvB6YHH6XdXtYOD2iLgvdSf/EcXfs1pFxLyI2CciZlE8zvhd3TF1KtektQjYSdKM9L+rtwMX1BxT15Ek4FvATRFxSt3xAEjaUtKm6fNGFP+4a+0YEhGfiojpEbEdRfPSryLi3XXGBEWHFUlT0ue/Av4GqLV3akqiKyW9NG06GLixxpBaHU7xn45ucAcwU9JG6d/hwRTvP9VK0gvSz5dQPM/qlt9XaRPrDmAkIuIJSf0vwk0AvtUFPeLmA7OALSStBD4bEWfVGRPw18A7geskXZu2HR8Rv6gxphcDZ6ceoOtRvMR4UY3xDKRbmppfCPy4KPOYCHwvIrph1oMPA99L/2G8DZhbczykZzQHA93w3I+IuErS+cA1wBPp57/XGxUA56eRLdYCH+yyTjSlZNnl3czMxqdcmwfNzGwcctIyM7NsOGmZmVk2nLTMzCwbTlpmZpYNJy0zM8uGk5bZGJLU001Tn5jlxknLzMyy4aRlNgBJ70yTVV4r6RtpgsiHJX1Z0mJJv5S0ZTp2L0n/JWmppB+1DFO1YzpuSTpne4rRNiZL+kGaRPG7Lff8Z0k3put8qZ5vbtbdnLTM2kjaBXgbxQjre1OMhH0ExZxIi9Oo65cDjXTKOcAnI2JPirEB+7d/DzgtjWj/auAeilHk9waOAXYFtpf015I2p5igc7d0nRPH4KuaZcdJy+zZDqKYwXhRGrPxdcB2FHNtnZeO+S6wf5oUcZM0CSjA2RRzFk0Gto6InwJExF8i4rF0zFURcXcUY6gtAbYFHgAel/RNSW8G+o81sxZOWmYDOzsi9k7LLhHRbNsvBh5YV20/B/Lnls/rgElpJtn9KKaROZRiKnQza+OkZfZsC4DDJG0FkGZ63Zbi38tb0zHvABZGxIPAGkn7p+3vAvoi4iFglaQ3pWtskKZiGVCaemTTiPg58DFgryq+mFnuspyaxKxKEbFM0qcpprlfD/gLcDTFLL27SVoE3E8xjxvAkcA30vQYrVN1vAs4Q9IJ6Rpvo6idtdfQApgC/FTShhS1tI9W9f3McuapScxKkvRQREypOw6z8czNg2bl+X94ZjVzTcvMzLLhmpaZmWXDScvMzLLhpGVmZtlw0jIzs2w4aZmZWTactMzMLBv/AzwcD5hMJh/IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1288c01d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datapath = 'data/mnist.pkl.npy'\n",
    "modelpath = 'model/mlp.pkl.npy'\n",
    "\n",
    "eta = 0.03\n",
    "batch_size = 500\n",
    "epoch = 10\n",
    "lmbda = 0\n",
    "\n",
    "mlp = NN(datapath=datapath, modelpath=modelpath, size_layers=[784, 10, 10, 10])\n",
    "\n",
    "initialize_zero_weights(mlp)\n",
    "losses, accuracies = train(mlp, eta, epoch=epoch, batch_size=batch_size, lmbda=lmbda)\n",
    "\n",
    "draw_plots(mlp, epoch, accuracies, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing normally distributed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datapath = 'data/mnist.pkl.npy'\n",
    "modelpath = 'model/mlp.pkl.npy'\n",
    "\n",
    "eta = 0.000045\n",
    "batch_size = 500\n",
    "epoch = 10\n",
    "lmbda = 0\n",
    "\n",
    "mlp = NN(datapath=datapath, modelpath=modelpath, size_layers=[784, 10, 10, 10])\n",
    "\n",
    "initialize_normal_weights(mlp)\n",
    "losses, accuracies = train(mlp, eta, epoch=epoch, batch_size=batch_size, lmbda=lmbda)\n",
    "\n",
    "draw_plots(mlp, epoch, accuracies, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing with glorot distributed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data/mnist.pkl.npy'\n",
    "modelpath = 'model/mlp.pkl.npy'\n",
    "\n",
    "eta = 0.0003\n",
    "batch_size = 500\n",
    "epoch = 10\n",
    "lmbda = 0\n",
    "\n",
    "mlp = NN(datapath=datapath, modelpath=modelpath, size_layers=[784, 16, 16, 10])\n",
    "\n",
    "initialize_glorot_weights(mlp)\n",
    "losses, accuracies = train(mlp, eta, epoch=epoch, batch_size=batch_size, lmbda=lmbda)\n",
    "\n",
    "draw_plots(mlp, epoch, accuracies, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data/mnist.pkl.npy'\n",
    "modelpath = 'model/mlp.pkl.npy'\n",
    "\n",
    "eta = 0.0003\n",
    "batch_size = 500\n",
    "epoch = 10\n",
    "lmbda = 0.00200\n",
    "\n",
    "mlp = NN(datapath=datapath, modelpath=modelpath, size_layers=[784, 16, 16, 10])\n",
    "\n",
    "initialize_glorot_weights(mlp)\n",
    "losses, accuracies = train(mlp, eta, epoch=epoch, batch_size=batch_size, lmbda=lmbda)\n",
    "\n",
    "draw_plots(mlp, epoch, accuracies, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Gradients using Finite Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, 5, 5)\n",
    "k = np.random.randint(1, 5, 5)\n",
    "\n",
    "N = [k*10^i for i, k in zip(i, k)]\n",
    "epsilons = [1/n for n in N]\n",
    "\n",
    "mlp = NN(datapath=datapath, modelpath=modelpath, size_layers=[784, 16, 16, 10])\n",
    "\n",
    "p = min(10, 16)\n",
    "\n",
    "train_set, valid_set, test_set = mlp.data\n",
    "\n",
    "X, Y = train_set\n",
    "\n",
    "x = X[0]\n",
    "y = Y[0]\n",
    "\n",
    "results = []\n",
    "for e, i, N, k in zip(epsilons, i, N, k):\n",
    "    initialize_glorot_weights(mlp)\n",
    "    mlp.theta['w'][1][:,p:] = 0\n",
    "    old_w = mlp.theta['w'][1][:,i]\n",
    "    \n",
    "    mlp.theta['w'][1][:,i] = old_w + e\n",
    "    Y_h = forward(mlp, x)\n",
    "    l_los = loss(mlp, Y_h, y)\n",
    "    \n",
    "    mlp.theta['w'][1][:,i] = old_w - e\n",
    "    Y_h = forward(mlp, x)\n",
    "    r_los = loss(mlp, Y_h, y)\n",
    "    \n",
    "    delta_N_i = (l_los - r_los)/(2*e)\n",
    "    results.append((e, i, k, N, delta_N_i))\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = range(1, 50)\n",
    "i = range(1, p)\n",
    "\n",
    "max_diff = []\n",
    "\n",
    "initialize_glorot_weights(mlp)\n",
    "\n",
    "for n in N:\n",
    "    e = 1/n\n",
    "    grad_diff = []\n",
    "    \n",
    "    for _i in i:\n",
    "        mlp.theta['w'][1][:,p:] = 0\n",
    "        old_w = mlp.theta['w'][1][:,_i]\n",
    "\n",
    "        mlp.theta['w'][1][:,_i] = old_w + e\n",
    "        Y_h = forward(mlp, x)\n",
    "        l_los = loss(mlp, Y_h, y)\n",
    "\n",
    "        mlp.theta['w'][1][:,_i] = old_w - e\n",
    "        Y_h = forward(mlp, x)\n",
    "        r_los = loss(mlp, Y_h, y)\n",
    "\n",
    "        delta_N_i = (l_los - r_los)/(2*e)\n",
    "        \n",
    "        mlp.theta['w'][1][:,_i] = old_w\n",
    "        mlp.theta['w'][1][:,p:] = 0\n",
    "        \n",
    "        forward(mlp, x)\n",
    "        backward(mlp, np.array([y]))\n",
    "        real_grad = mlp.grads['w'][1]\n",
    "        \n",
    "        grad_diff.append(abs(delta_N_i - real_grad))\n",
    "    \n",
    "    max_diff.append(np.max(grad_diff))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(N, max_diff)\n",
    "plt.xlabel('N') \n",
    "plt.title('abs(true grad - finite diff)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [1, 3, 6, 8],\n",
    "    [2, 4, 7, 9]\n",
    "])\n",
    "y = np.array([ \n",
    "    0, \n",
    "    1\n",
    "])\n",
    "\n",
    "mlp_experience = NN(datapath=datapath, modelpath=modelpath, size_layers=[4, 2, 2])\n",
    "initialize_glorot_weights(mlp_experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [1, 3, 6, 8],\n",
    "    [2, 4, 7, 9]\n",
    "])\n",
    "y = np.array([ \n",
    "    0, \n",
    "    0\n",
    "])\n",
    "y_h = forward(mlp_experience, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0.50037769*2 + 0.52710657*4 -0.47977167*7 + 0.84972747*9)\n",
    "print(0.34739177*2 -0.78948968*4 +  0.61106684*7 +  0.1246129*9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-0.06578369*7.39832714 - 0.56865302*2.93580881)\n",
    "print(-0.70030729*7.39832714 + 0.13967644*2.93580881)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_experience.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_experience.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward(mlp_experience, y)\n",
    "mlp_experience.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(mlp_experience, y_h, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
