{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1701,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('error')\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self, size_layers=[784, 16, 16, 10], datapath=None, modelpath=None):\n",
    "        self.data = np.load(datapath)\n",
    "        self.size_layers = size_layers\n",
    "        \n",
    "        # The cache would be used to store forward feed activations and preactivations values \n",
    "        self.cache = {\n",
    "            'activations': [],\n",
    "            'preactivations': []\n",
    "        }\n",
    "        \n",
    "        # Data structure to hold the params of the model\n",
    "        self.theta = {\n",
    "            'b': [],\n",
    "            'w': []\n",
    "        }\n",
    "        \n",
    "        # Data structure to hold the gradients calculated during backprop\n",
    "        self.grads = {\n",
    "            'b': [],\n",
    "            'w': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_zero_weights(self):\n",
    "    size_next_layers = self.size_layers.copy()\n",
    "    size_next_layers.pop(0)\n",
    "    \n",
    "    for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "        self.theta['w'].append(np.zeros((size_next_layer, size_layer)))\n",
    "        self.theta['b'].append(np.zeros((1, size_next_layer)))\n",
    "        \n",
    "        self.grads['w'].append(np.zeros((size_next_layer, size_layer)))\n",
    "        self.grads['b'].append(np.zeros((1, size_next_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1703,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_normal_weights(self):\n",
    "    size_next_layers = self.size_layers.copy()\n",
    "    size_next_layers.pop(0)\n",
    "\n",
    "    for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "        self.theta['w'].append(np.random.normal(size=(size_next_layer, size_layer)))\n",
    "        self.theta['b'].append(np.zeros((1, size_next_layer)))\n",
    "\n",
    "        self.grads['w'].append(np.random.normal(size=(size_next_layer, size_layer)))\n",
    "        self.grads['b'].append(np.zeros((1, size_next_layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1704,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_glorot_weights(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X):\n",
    "    '''\n",
    "    Forward propagation\n",
    "    params:\n",
    "        X: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of sample\n",
    "    returns: output is a N by o matrix where o is the output dimension specified\n",
    "    in mlp initialization\n",
    "    '''\n",
    "    n_layers = len(self.size_layers)\n",
    "    input_layer = X\n",
    "    \n",
    "    # Adding input in activations needed for gradient descent\n",
    "    self.cache['activations'].append(X)\n",
    "    \n",
    "    for layer_idx in range(n_layers - 2):\n",
    "        \n",
    "        # Multiply the input by the weights\n",
    "        pre_act_layer = np.matmul(input_layer,  self.theta['w'][layer_idx].transpose()) + self.theta['b'][layer_idx]\n",
    "        # Apply activation function\n",
    "        output_layer = activation(self, pre_act_layer)\n",
    "        \n",
    "        self.cache['preactivations'].append(pre_act_layer)\n",
    "        self.cache['activations'].append(output_layer)\n",
    "        \n",
    "        input_layer = output_layer\n",
    "    \n",
    "    # Final layer with softmax activation\n",
    "    # Multiply the input by the weights\n",
    "    pre_act_layer = np.matmul(input_layer,  self.theta['w'][layer_idx + 1].transpose()) + self.theta['b'][layer_idx + 1]\n",
    "    # Apply activation function\n",
    "    output_layer = softmax(self, pre_act_layer)\n",
    "\n",
    "    self.cache['preactivations'].append(pre_act_layer)\n",
    "    self.cache['activations'].append(output_layer)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1706,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(self, input):\n",
    "    zero_aux = np.zeros(input.shape)\n",
    "    meta_z = np.stack((input , zero_aux), axis = -1)\n",
    "    return np.max(meta_z, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1707,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_prime(self, input):\n",
    "    result = input.copy()\n",
    "    result[result<=0] = 0\n",
    "    result[result>0] = 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1708,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self, input):\n",
    "    '''\n",
    "    params:\n",
    "        input: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of each sample\n",
    "    returns: a N by k matrix representing the probability distribution \n",
    "        of each sample\n",
    "    '''\n",
    "    e_x = np.exp(input - np.max(input, axis=1, keepdims=1))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1709,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, predictions, targets):\n",
    "    '''\n",
    "    Takes a batch of predictions and gets the cross entropy\n",
    "    params:\n",
    "        predictions: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of each sample\n",
    "        targets: N by 1 matrix containing label of each one\n",
    "        where \n",
    "            N: number of samples\n",
    "    returns: a scalar which is the loss value            \n",
    "    '''\n",
    "    N = predictions.shape[0] # batch size\n",
    "    k = predictions.shape[1] # numb of classes\n",
    "    \n",
    "    return -np.sum(np.eye(k)[targets]*np.log(predictions))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_prime(self, predictions, targets):\n",
    "    N = predictions.shape[0] # batch size\n",
    "    k = predictions.shape[1] # numb of classes\n",
    "    \n",
    "    return np.sum(predictions - np.eye(k)[targets], axis=0, keepdims=1)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1711,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, Y):\n",
    "    '''\n",
    "    Backward propagation\n",
    "    params:\n",
    "        Y: N by 1 matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            column: is the label of the samples\n",
    "    '''\n",
    "    N = Y.shape[0]\n",
    "    n_layers = len(self.size_layers)\n",
    "    \n",
    "    delta = loss_prime(self, self.cache['activations'][-1], Y)\n",
    "    \n",
    "    self.grads['b'][-1] = delta\n",
    "    self.grads['w'][-1] = np.dot(delta.transpose(), np.sum(self.cache['activations'][-2], axis=0, keepdims=1)/N)\n",
    "    \n",
    "    for l in range(2, n_layers):\n",
    "        z = self.cache['preactivations'][-l]\n",
    "        o = activation_prime(self, z)\n",
    "        \n",
    "        delta = np.dot(delta, self.theta['w'][-l+1])*(np.sum(o, axis=0, keepdims=1)/N)\n",
    "        \n",
    "        self.grads['b'][-l] = delta\n",
    "        self.grads['w'][-l] = np.dot(delta.transpose(), np.sum(self.cache['activations'][-l-1], axis=0, keepdims=1)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    n_layers = len(self.size_layers)\n",
    "    \n",
    "    for layer_idx in range(n_layers - 1):\n",
    "        self.theta['w'][layer_idx] = self.theta['w'][layer_idx] - eta*self.grads['w'][layer_idx]\n",
    "        self.theta['b'][layer_idx] = self.theta['b'][layer_idx] - eta*self.grads['b'][layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, eta, epoch=10, batch_size=1000):\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        train_set, valid_set, test_set = self.data\n",
    "        \n",
    "        X, Y = train_set\n",
    "        \n",
    "        mini_batches = [\n",
    "            (X[k:k+batch_size], Y[k:k+batch_size]) for k in range(0, len(X), batch_size)\n",
    "        ]\n",
    "        \n",
    "        for X, Y in mini_batches:\n",
    "            forward(mlp, X)\n",
    "            backward(mlp, Y)\n",
    "            update(mlp, eta)\n",
    "        \n",
    "        Y_h = forward(mlp, X)\n",
    "        try:\n",
    "            losses.append(loss(mlp, Y_h, Y))\n",
    "        except Warning:\n",
    "            print(Y_h)\n",
    "            print(Y)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1714,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question1\n",
    "1.\n",
    "\n",
    "Initializing weights to zero, with 0.5 as learning rate and no minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1715,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = 'data/mnist.pkl.npy'\n",
    "# modelpath = 'model/mlp.pkl.npy'\n",
    "\n",
    "# eta = 0.5\n",
    "# mlp = NN(datapath=datapath, modelpath=modelpath)\n",
    "\n",
    "# initialize_zero_weights(mlp)\n",
    "# losses = train(mlp, eta)\n",
    "\n",
    "# plt.plot(range(len(losses)), losses)\n",
    "# plt.xlabel('epochs') \n",
    "# plt.title('Loss function')\n",
    "# plt.axhline(0, color='white')\n",
    "# plt.axvline(0, color='white')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing normally distributed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1716,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1114c5c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datapath = 'data/mnist.pkl.npy'\n",
    "modelpath = 'model/mlp.pkl.npy'\n",
    "\n",
    "eta = 0.5\n",
    "mlp = NN(datapath=datapath, modelpath=modelpath)\n",
    "\n",
    "initialize_normal_weights(mlp)\n",
    "losses = train(mlp, eta)\n",
    "\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.xlabel('epochs') \n",
    "plt.title('Loss function')\n",
    "plt.axhline(0, color='white')\n",
    "plt.axvline(0, color='white')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
