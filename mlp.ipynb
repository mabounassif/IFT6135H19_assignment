{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NN(object):\n",
    "    def __init__(self, size_layers=[784, 16, 16, 10], datapath=None, modelpath=None):\n",
    "        self.data = np.load(datapath)\n",
    "        self.size_layers = size_layers\n",
    "        \n",
    "        # The cache would be used to store forward feed activations and preactivations values \n",
    "        self.cache = {\n",
    "            'activations': [],\n",
    "            'preactivations': []\n",
    "        }\n",
    "        \n",
    "        # Data structure to hold the params of the model\n",
    "        self.theta = {\n",
    "            'b': [],\n",
    "            'w': []\n",
    "        }\n",
    "        \n",
    "        # Data structure to hold the gradients calculated during backprop\n",
    "        self.grads = {\n",
    "            'b': [],\n",
    "            'w': []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(self):\n",
    "    size_next_layers = self.size_layers.copy()\n",
    "    size_next_layers.pop(0)\n",
    "    \n",
    "    for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "        self.theta['w'].append(np.zeros((size_next_layer, size_layer)))\n",
    "        self.theta['b'].append(np.zeros((size_next_layer, 1)))\n",
    "        \n",
    "        self.grads['w'].append(np.zeros((size_next_layer, size_layer)))\n",
    "        self.grads['b'].append(np.zeros((size_next_layer, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X):\n",
    "    '''\n",
    "    Forward propagation\n",
    "    params:\n",
    "        X: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of sample\n",
    "    returns: output is a N by o matrix where o is the output dimension specified\n",
    "    in mlp initialization\n",
    "    '''\n",
    "    n_layers = len(self.size_layers)\n",
    "    input_layer = X\n",
    "    \n",
    "    # Adding input in activations needed for gradient descent\n",
    "    self.cache['activations'].append(X)\n",
    "    \n",
    "    for layer_idx in range(n_layers - 1):\n",
    "        n_examples = input_layer.shape[0]\n",
    "        \n",
    "        # Multiply the input by the weights\n",
    "        pre_act_layer = np.matmul(input_layer,  self.theta['w'][layer_idx].transpose()) + self.theta['b'][layer_idx].transpose()\n",
    "        # Apply activation function\n",
    "        output_layer = activation(self, pre_act_layer)\n",
    "        \n",
    "        self.cache['preactivations'].append(pre_act_layer)\n",
    "        self.cache['activations'].append(output_layer)\n",
    "        \n",
    "        input_layer = output_layer\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(self, input):\n",
    "    '''\n",
    "    Rectified Linear function\n",
    "    z can be an numpy array or scalar\n",
    "    '''\n",
    "    if np.isscalar(input):\n",
    "        result = np.max((input, 0))\n",
    "    else:\n",
    "        zero_aux = np.zeros(input.shape)\n",
    "        meta_z = np.stack((input , zero_aux), axis = -1)\n",
    "        result = np.max(meta_z, axis = -1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_prime(self, input):\n",
    "    input[input<=0] = 0\n",
    "    input[input>0] = 1\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, predictions, targets):\n",
    "    '''\n",
    "    Takes a batch of predictions and gets the cross entropy\n",
    "    params:\n",
    "        predictions: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of each sample\n",
    "        targets: N by 1 matrix containing label of each one\n",
    "        where \n",
    "            N: number of samples\n",
    "    returns: a scalar which is the loss value            \n",
    "    '''\n",
    "    N = predictions.shape[0] # batch size\n",
    "    k = predictions.shape[1] # numb of classes\n",
    "    \n",
    "    return -np.sum(np.eye(k)[targets]*np.log(predictions))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_prime(self, predictions, targets):\n",
    "    '''\n",
    "    Derivative of the cross entropy\n",
    "    params:\n",
    "        predictions: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of each sample\n",
    "        targets: N by 1 matrix containing label of each one\n",
    "        where \n",
    "            N: number of samples\n",
    "    returns: a scalar which is the loss value            \n",
    "    '''\n",
    "    N = predictions.shape[0] # batch size\n",
    "    k = predictions.shape[1] # numb of classes\n",
    "    epsilon = 0.000001 # needed when weights are all zeros\n",
    "    \n",
    "    return -np.sum(1/(epsilon + np.eye(k)[targets]*predictions))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self, input):\n",
    "    '''\n",
    "    params:\n",
    "        input: N by k matrix\n",
    "        where\n",
    "            N: number of samples\n",
    "            k: dimension of each sample\n",
    "    returns: a N by k matrix representing the probability distribution \n",
    "        of each sample\n",
    "    '''\n",
    "    e_x = np.exp(input.T - np.max(input))\n",
    "    return (e_x / e_x.sum(axis=0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_prime(self, input):\n",
    "    return np.diag(softmax(self, input)- np.outer(softmax(self, input), softmax(self, input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, Y):\n",
    "    N = Y.shape[0]\n",
    "    n_layers = len(self.size_layers)\n",
    "    \n",
    "    delta = np.sum(loss_prime(self, self.cache['activations'][-1], Y)*activation_prime(self, self.cache['preactivations'][-1]))/N\n",
    "\n",
    "    self.grads['b'][-1] = delta\n",
    "    self.grads['w'][-1] = np.sum(delta*self.cache['activations'][-2])/N\n",
    "    \n",
    "    for l in range(2, n_layers):\n",
    "        z = self.cache['preactivations'][-l]\n",
    "        o = activation_prime(self, z)\n",
    "        \n",
    "        delta = np.sum(np.matmul(o, self.theta['w'][-l+1].transpose() * delta))\n",
    "        \n",
    "        self.grads['b'][l] = delta\n",
    "        self.grads['w'][l] = delta * self.cache['activations'][-l-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self,grads):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    train_set, valid_set, test_set = self.data\n",
    "    \n",
    "    X, Y = train_set\n",
    "    \n",
    "    forward(mlp, X)\n",
    "    backward(mlp, Y)\n",
    "    \n",
    "    mlp.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-610-806e10b312ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minitialize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-608-9ee4c36408b1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-606-adc94c494f0a>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, Y)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "datapath = 'data/mnist.pkl.npy'\n",
    "modelpath = 'model/mlp.pkl.npy'\n",
    "\n",
    "mlp = NN(datapath=datapath, modelpath=modelpath)\n",
    "\n",
    "initialize_weights(mlp)\n",
    "train(mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
